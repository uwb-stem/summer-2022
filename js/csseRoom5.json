{
    "csse": [
        {
            "time": "12:45 PM - 1:00 PM",
            "projectId": "csse-5-1245",
            "title": "TFeat Feature Descriptor & HPatches Dataset in OpenCV",
            "studentName": "Zachary Hanneman",
            "studentMajor": "CSSE",
            "projectType": "Faculty Research",
            "facultyAdvisor": "Dr. Clark Olson",
            "posterLink": "./posters/csse/hanneman_zach.png",
            "abstract": "Local feature descriptors aim to encode interesting information about a keypoint within an image in a way that attempts to be invariant to image transformations and illumination changes. Computing the set of feature descriptors for a set of keypoints within an image, typically found using a separate feature detector, can allow for useful applications such as matching objects between multiple images where the images have changed from the original to some degree. Local feature descriptors based on convolutional neural networks have demonstrated their ability to improve keypoint matching performance over some traditional descriptor algorithms.\n\nThe TFeat local feature descriptor obtains matching performance comparable to other CNN-based descriptors, while maintaining speeds comparable to traditional non-CNN descriptors. However, due to it being trained and implemented in PyTorch using Python, it previously did not function well within a C++ OpenCV framework environment, relying on messy calls over to the original python implementation to run the descriptor within a testing environment. Through this project’s work, the descriptor’s pretrained models were ported over to the ONNX format through the development of a conversion tool in Python, allowing them to be properly read and loaded by OpenCV’s DNN module. Then, the behavior of the TFeat’s pre-processing of patches around keypoints involving applying scaling and rotational warping to the patches according to keypoint parameters was implemented in order to match the conditions the original implementation had prior to forwarding the patches through the neural network.\n\nDescriptor performance is often evaluated through the computation of the keypoints for sequences of images belonging to some dataset, their descriptors, then matching those between the images. Matches are then compared to ground-truth matches, allowing for the calculation of the mean average precision of the descriptor’s matches, an important performance evaluation statistic. However, these datasets depend on a separate detector to compute the keypoints to compute descriptors for, adding another variable whose choice can significantly impact descriptor performance results. The HPatches dataset eliminates this issue through replacing sequence images with series of pre-extracted patches corresponding to keypoints computed using several different detection methods. This allows for the dataset to generate reproducible descriptor matching results independent of feature detector choice. Through this capstone project’s work, the dataset’s matching task behavior was ported to OpenCV’s C++ environment, permitting the improvement of the accuracy and reproducibility of the performance evaluations of the different feature descriptors implemented in OpenCV."
        },
        {
            "time": "1:00 PM - 1:15 PM",
            "projectId": "csse-5-100",
            "title": "Real Estate Information System",
            "studentName": "Preston Parsons",
            "studentMajor": "CSSE",
            "projectType": "Individual Project",
            "facultyAdvisor": "Dr. Clark Olson",
            "posterLink": "./posters/csse/parsons_preston_james.png",
            "abstract": "For my capstone, I chose to do an individual project with Clark Olson. The main objective was to create an information system that provides a greater variety of search attributes for real estate agents. With a focus on data engineering, I forged a connection between the two biggest property data sources: raw government data and current market data.\n\nOne of the main reasons for this project is to give real estate agents a better way to gather information, rather than the usual sources of current market websites or interviewing people face-to-face. There is a problem preventing this from happening. While the complete data is publicly available online in county assessor’s offices, it is raw, requiring the real estate agent to have the skills of a data engineer to process and connect this raw data with properties currently on the market. Each city has unique data, meaning locals that use accessor data have an advantage. Although this data tends to be used by government officials, many real estate agents require this hidden data to sort by specialized criteria.\n\nAfter ten weeks of development, I successfully created a working prototype of the information system. I built the back end using Python and SQL, with additional processing in QGIS. The workflow takes users through a three-step process to process the data. First, current market data is web scraped from an online brokerage and updates the SQL database automatically via Python. Second, geospatial data from government assessor’s offices is processed using vector queries in an open-source program called QGIS. Thirdly the symbol decoding table is converted from city documentation to excel manually. After this is complete, the collected data is combined in SQL, where users query for  desirable properties. Once the preliminary data is processed for a particular location, the web scraper keeps the database updated in real time with property market data. Currently, the prototype only works with 2 different locations I selected, but future work would allow the tool to work with any location.\n\nInformation systems for property data exist in popular websites such as Redfin and Zillow, but nothing like this exists to dive deep in specific locations, beyond the layer of generalized market data. With future work involving other data engineering/analytics techniques, SQL queries, and web scraping, this project will help real estate agents of all kinds create queries using a greater variety of search attributes."
        },
        {
            "time": "1:15 PM - 1:30 PM",
            "projectId": "csse-5-115",
            "title": "Information Security Internship",
            "studentName": "Hanat Samatar",
            "studentMajor": "CSSE",
            "projectType": "Internship - Harvey Nash Rover",
            "facultyAdvisor": "Dr. Clark Olson",
            "posterLink": "./posters/csse/samatar-hanat.png",
            "abstract": "During my Security Internship with Rover, I worked with the Information Security Team and many others (Payments and Site Reliability) to secure the companies information and complete a variety of security related projects. One of the projects I prioritized on was automizing our quarterly access reviews that require compliance manual overhead for periods of time that add up to eventually be too time consuming. \n\nThe purpose of the automation scripts was to expedite any processes requiring an outside auditor. As a result, I was able to automate a subset of sensitive systems for the company, allowing members of our compliance team to forward that information over to auditors. I chose to use Python for my script work since there are a lot of libraries to help simplify many processes. The intent of automating the sensitive systems was to help everyone in the company expedite how they report which users have access to certain applications or infrastructures within the company. The significance of this main project was to drastically cut manual work time each quarter by many hours and over-time save the company days of work time.\n\n Another project I worked on was addressing account takeovers. During my internship there were a subset of account takeover attempts in which I worked with the security analyst and the rest of the team to address and resolve the problems. Along with this project, another required me to organize and send out the vulnerabilities left over by a security contractor to the correct departments and specific teams to finalize those tasks. This was expedited using specific endpoints and Excel/CSV files with the departments and head of each department on the files for sorting.\n\n Overall, these projects and the daily events that occurred throughout the internship implemented in me a great understanding of a professional engineering environment and how a company expects information and security to be handled across their organization and engineering teams. "
        },
        {
            "time": "1:30 PM - 1:45 PM",
            "projectId": "csse-5-130",
            "title": "Mozzign Software Engineer Internship",
            "studentName": "Josh Kweon",
            "studentMajor": "CSSE",
            "projectType": "Internship - Mozzign",
            "facultyAdvisor": "Dr. Annuska Zolyomi",
            "posterLink": "./posters/csse/kweon_josh.jpg",
            "abstract": "For the Capstone project, I worked as a software engineer intern at Mozzign for the last two quarters. This capstone is sponsored by Mozzgin. Mozzign is a modern full-service web development and interactive web design agency in New York. Founded in 2007, the company is focusing on bringing out one firm concept and applying consistent and powerful strategies.  During my internship, my personal goals were to apply the skills and knowledge gained in UWB CSS classes and expand my industry knowledge in software engineering.\n\nDuring the first three months, most of the time was spent on the mobile application team, applying Software Development Life Cycle methodologies (Agile) for basic application structures and effective development. Also, to design UI/UX, I wrote code in React.js and used Visual Studio Code, the default IDE. In this task, I configured the UI/UX of the first home screen displayed when the application is executed.\n\nFor another 3 months, my main task was to code the login process, such as using TypeScript to verify that the user entered a valid email and password under the appropriate conditions. The website on which the project is based used Joomla, a content management system (CMS), which is an open-source platform that allows developers to easily build dynamic and responsive websites and web applications. Joomla uses MySQL to store large amounts of digital content collected through its website, and I queried and fetched data from the database to collect information about users. After logging in, the user's ID card is digitized to provide the user's basic information, and a code for each unique access processing is written using the QR code generation library provided by TypeScript. Also, I demonstrated the first demo version of the application in July and managed a matrix for error checking and improvement.\n\nDuring my internship, the skills that I learned at UWB were very useful, and the opportunity to use them was precious. I believe that the knowledge and wisdom gained from this capstone project will help you prepare and execute your future career. More details will be provided in the presentation."
        },
        {
            "time": "1:45 PM - 2:00 PM",
            "projectId": "csse-5-145",
            "title": "Passwordless Authentication using AWS Services",
            "studentName": "Eunsol Cho",
            "studentMajor": "CSSE",
            "projectType": "Internship - Amazon",
            "facultyAdvisor": "Dr. Annuska Zolyomi",
            "posterLink": "./posters/csse/cho_eunsol.jpg",
            "abstract": "The project that I worked on as an intern at AWS was about implementing Passwordless Authentication using AWS Cognito and Amplify. Remembering passwords can be a pain, especially for passwords that are not often used. Like most people, you're probably used to clicking on 'Forget Your Password' links or buttons on websites and apps. Many people try to use bad practices such as using short passwords, using easily guessable passwords, and reusing the same passwords on many sites and apps. There is a solution for this (such as a password manager), but in practice, password-based security is not that secure and user-friendly. There are alternatives to logging in with passwords—for example, using a fingerprint scan or facial recognition. However, using such a method is not always feasible. Amazon Cognito can provide another alternative. With Amazon Cognito user pools, I have created a custom authentication flow that sends an one-time login code to the user’s email. The passwordless email authentication solution used an Amazon Cognito user pool and a couple of Lambda functions. Also, the Amazon Simple Email Service (SES) for sending the emails with the one-time log-in code was used. Also, the log-in process was supported by custom UI pages using AWS Amplify, Typescript, and React. Throughout the internship, I was able to successfully implement passwordless authentication into AWS internal portal. Through working on a project, I was able to learn various AWS services, new technologies, and languages such as Typescript and React, and I was able to experience the Amazon work culture and environment.  "
        },
        {
            "time": "2:00 PM - 2:15 PM",
            "projectId": "csse-5-200",
            "title": "Internship @ Amazon",
            "studentName": "Alexander Rangel",
            "studentMajor": "CSSE",
            "projectType": "Internship - Amazon",
            "facultyAdvisor": "Dr. Annuska Zolyomi",
            "posterLink": "./posters/csse/rangel_alexander.JPG",
            "abstract": "Made a display to measure the customer usage and performance of the cloud. In making this project I had to retrieve data from the cloud and perform analysis on it. The analysis looked into where performance could be improved and the overall status of the cloud. On the display there were also filters, custom fields, and calculations that were all geared towards helping customers understand the performance of the cloud.  \n\nThis display was needed due to an increase in cloud usage and so the company wanted to know where improvements could be made. There are other ways to get the cloud performance, but none of them are nearly as convenient or streamlined as a display would be. A wide variety of customers would be supported with this display instead of all of them having their own unique way to find the performance metrics.  \n\nAfter completing the display, customers are now more knowledgeable on the cloud performance. Instead of guessing and using theoretical concepts to improve cloud performance, customers can now base their decisions on data. This allows them to make optimal solutions and back up their decisions when allocating time for improvements. Having the display will allow customers to move the product forward and prevent deficient solutions. "
        },
        {
            "time": "2:15 PM - 2:30 PM",
            "projectId": "csse-5-215",
            "title": "Usability Engineer at Command+Fi",
            "studentName": "Bailey Schuler",
            "studentMajor": "CSSE",
            "projectType": "Internship - Command+Fi",
            "facultyAdvisor": "Dr. Annuska Zolyomi",
            "posterLink": "./posters/csse/schuler_bailey_marie.png",
            "abstract": "What:\n\nI worked with a local startup that matches car loans to users through an online system to reduce bias and make gaining a car loan easier for those who seek one. In this project I created key personas to isolate users of the car loan matching to user system. With these personas I did research and usability studies to understand what was working efficiently and effectively for the user and what was difficult about the system or not user-friendly.\n\nWhy:\n\nThe business owners are not experienced in user research or usability engineering and wanted to make a user-first product, so they wanted me to test the usability of their system. This startup is relatively new, and they needed help understanding who their key customers/users would be and who is their direct competition. This startup also needs to understand the user-flow of their system and what is needed to make their system the most compatible with all users.\n\nResult:\n\nThe result of this project is gaining the data of key performance indicators within the loan system for users. I identified key user personas, developed user journey maps, conducted heuristic reviews and usability tests that gained data and knowledge from a user standpoint. I performed competitive analysis and SWOT analysis for the start-ups competition to understand what their competitor’s strengths and weaknesses were.\n\nSignificance:\n\nUsability engineering is important to any product that has users. Products and system that put users first and focus on users being able to accurately perform tasks within their products tend to curate products that succeed in the market."
        },
        {
            "time": "2:30 PM - 2:45 PM",
            "projectId": "csse-5-230",
            "title": "Creating a Contract Testing Process",
            "studentName": "Xavier Cano",
            "studentMajor": "CSSE",
            "projectType": "Internship - Qualtrics",
            "facultyAdvisor": "Dr. William Erdly",
            "posterLink": "./posters/csse/cano_xavi.png",
            "abstract": "The problem I was looking to solve was that many production crashes had been occurring as a result of changes being pushed without checking with all the services that depend on it. This would often occur because one team decides to use another team’s product or tool without notifying them. The proposed solution was contract testing which requires that the provider of a tool must check with the consumer of a tool that changes would not cause any issues. I made a document that serves as a guide to future developers on how contract testing should be implemented. I then followed my process multiple times in production codebases to show that the process works and to serve as examples for other developers to use.\n\nThe purpose of my work was to reduce the amount crashes as a result of one team using software made by another team without informing them. Contract testing is incredibly useful because both parties agree to a contract of expectations to ensure that there isn’t a mismatch between them. While I cannot disclose the amount of money that can be saved by catching these errors before they go into production and eventually cause a crash, I can disclose that is incredibly worthwhile to implement contract testing organization wide. The end result is not yet known as my internship has not yet ended. Due to my efforts, there is an expected decrease in downtime and production crashes. The end user should notice less interruptions and bugs while using any of the products used at the company I interned at."
        },
        {
            "time": "2:45 PM - 3:00 PM",
            "projectId": "csse-5-245",
            "title": "Client Delivery: Neudesic IBM Intern",
            "studentName": "Tristan Cortez",
            "studentMajor": "CSSE",
            "projectType": "Internship - Neudesic",
            "facultyAdvisor": "Dr. William Erdly",
            "posterLink": "./posters/csse/cortez_tristan2.jpg",
            "abstract": "On the start of summer 2022, I had the opportunity to enter a full-time internship at Neudesic. The company is a consulting firm under IBM specializing in cloud services such as Microsoft Azure. During the 10-week duration I was working on a project using Flutter, a cross-platform framework developed by Google. \n\nThe project was for a client and we would follow the Scrum SDLC defining 2-week sprints with daily morning standups. Every other week we would have a planning/retrospective meeting and a backlog refinement meeting (grooming session). Each story would have a what, why, how, which are detailed specifications to other story and tasks created underneath. It would be the responsibility of the developer to create and manage the task’s status.\n\nDue to the restrictions of the NDA, the project details cannot be shared however the project went through a rehaul of specific components of the UI allowing it to appear modern following Google’s Material design.\n\n The significance of the work I have completed allows Neudesic to greatly contribute to reducing the application’s time to market. The experience has allowed me to understand the process of setting code up to be production ready. Learning code reviews, advanced git control, and Azure DevOps has further increased my level of respect for the process of software development."
        },
        {
            "time": "3:00 PM - 3:15 PM",
            "projectId": "csse-5-300",
            "title": "Educating Young Eye Research Group Screening Process",
            "studentName": "Euan Canoy",
            "studentMajor": "CSSE",
            "projectType": "Faculty Research",
            "facultyAdvisor": "Dr. William Erdly",
            "posterLink": "./posters/csse/canoy_euan_james.jpg",
            "abstract": "The Educating Young Eyes (EYE) Research Group is an organization that focuses on promoting and providing eye care through the latest technology for children in underserved communities. We aim to achieve this by using the latest technology with the help of the Near Vision Institute (NVI).\n\nThe goal of my capstone was to create a consent screening process that would support all the students getting free vision screening at the Auburn School District Welcome to School event. The consent process would require the student’s parent or guardian to approve receiving a vision screening from NVI and for them to enter all their student's basic information through a tablet. The process should also allow multiple users to complete a form on multiple devices.\n\nThe final version of the consent screening process is cloud supported and features a user-friendly UI and a database that securely stores student information. The consent screening process completed the Welcome to School event, successfully screening 77 families with no problems.\n\n The new consent screening process allows for quick and easy screening that supports future events. Few changes are needed to configure the pages to meet the needs of other events or school districts. \n\nThrough completing my capstone, I learned many things. I learned how to implement backend systems that support larger data sets and how to support applications using AWS. I also learned how to work and communicate within a team using a scrum developmental process. By applying the scrum process, I focused on developing a user interface using an iterative approach."
        }               
    ]
}