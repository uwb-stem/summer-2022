{
    "csse": [
        {
            "time": "12:45 PM - 1:00 PM",
            "projectId": "csse-3-1245",
            "title": "Slalom Build Internship",
            "studentName": "Kaveh Buenaventura",
            "studentMajor": "CSSE",
            "projectType": "Internship - Slalom",
            "facultyAdvisor": "Dr. Arkady Retik",
            "posterLink": "./posters/csse/buenaventura_kaveh.png",
            "abstract": "For my capstone project, I interned with Slalom Build as a Platform Engineer aligned with the Boston Build Center. As part of my internship, I worked with a team of 5 other interns in an Agile environment to create a notifications microservice for a personal development tool to be used by the company. Prior to my involvement in the project, the website we were adding our microservice to already existed and had basic functionality related to its scope. However, the only form of notifications being sent were email notifications. The problem statement by our product owner could be summarized as follows: “When a user makes a change on the platform, the person who needs to respond to that change gets one email. This fire-and-forget approach means that the user has no way of knowing if the responder has taken action and has no easy tool to remind them to take action”. As a team, we were to add a frontend notification component and the functional microservice built on AWS.\n\nBeing the only Platform Engineer intern on the team, I worked with AWS CloudFormation and SAM to create AWS resources programmatically via infrastructure as code. I also managed configuration files to personalize the development environment for the developers and expanded the CI/CD pipeline to cover what we as a team implemented during the project timeframe. I also got to experience common Agile ceremonies including daily standups, backlog refinement, sprint planning, and sprint retrospective. We were also able to demo our work at the end of each sprint including a final demo in front of the entire Boston Build Center.\n\nDuring this internship I got to learn AWS and DevOps related toolsets that I hadn’t learned in previous classes. I also got the opportunity to refine my git skills, presentation skills, knowledge of working in an Agile environment. I really enjoyed my time interning at Slalom Build and feel that I have grown not only my toolset and career, but my network of engineers that I can communicate with. I got to form meaningful connections with my teammates as we worked and debugged our way through the project, as well as with the more experienced developers that were there to guide us throughout the internship."
        },
        {
            "time": "1:00 PM - 1:15 PM",
            "projectId": "csse-3-100",
            "title": "File Processing Mircoservice for Internal Customers",
            "studentName": "Joseph Lan",
            "studentMajor": "CSSE",
            "projectType": "Internship - Nordstrom",
            "facultyAdvisor": "Dr. Arkady Retik",
            "posterLink": "./posters/csse/lan_joseph.png",
            "abstract": "My capstone internship project was focused around adding missing functionality to our business software service aimed at modernizing the processing and distribution of incoming business files from legacy and custom monolithic software solutions into a single well-controlled distributed microservice architecture solution.\n\nDuring the internship I analyzed the business requirements for processing a required file type our service was not currently supporting and designed a solution based on our existing software infrastructure to process and publish the contents of the file type to internal customers who wanted to consume the file information.\n\nDuring the project I ran into multiple conflicts and ambiguities between the initial plan to implement the feature and what was possible given our business requirements regarding Kafka, data schemas, and customers needs. I collaborated with different software business teams, our company engineering standards, and online software support documentation to design a new solution that would allow us to process the files while meeting our internal customer needs and business requirements.\n\nI was able to perform my program analysis, initial implementation, conflict resolution, refactoring, end-to-end development testing, non-production testing, and in-production testing for the new feature in eight weeks. This feature gave the software service and organization increased visibility into vital business information that was being missed previously for internal business groups that needed the information as well as provided data to our internal data analytics team."
        },
        {
            "time": "1:15 PM - 1:30 PM",
            "projectId": "csse-3-115",
            "title": "Recommended Brands for iOS Mobile Application",
            "studentName": "Thomas Su",
            "studentMajor": "CSSE",
            "projectType": "Internship - Nordstrom",
            "facultyAdvisor": "Dr. Arkady Retik",
            "posterLink": "./posters/csse/su_thomas_ying.png",
            "abstract": "I spent the summer working for a Seattle based retail company. I joined the company as an iOS engineering intern and helped improve the customer experience on the mobile application. Throughout the various projects I worked on I learned so many new things through the help of my mentors and other engineers on my team. The main project I worked on for my internship project included implementing a brand-new feature called recommended brands.\n\nSome of the responsibilities I had as an intern doing iOS development included doing pair programming, working on tickets in relation to the customer experience with navigating, searching, and browsing throughout the application, attending daily squad stand-ups, sprint retros and planning, grooming and estimation, usability testing, and presenting at the bi-weekly sprint demos, weekly one on ones with my manager, project mentor, and day-to-day mentor, and lastly learning different technologies and architectures through work on the iOS codebase.\n\nSpecifically, for my project I utilized technologies such as Xcode which is the iOS IDE, Swift which is the iOS programming language, Jenkins for creating testing builds, Git for version control and merge requests, the Jira board for sprints and tickets tracking, Optimizely for experiments and Configman for feature flags, platform validator for impression, engagement, and variation exposed events tracking, Charles for proxy debugging, and lastly New Relic for analytics.\n\nMy project included implementing a new endpoint call, making front end UI changes, sending analytics, and writing unit and integration tests. It also included using the domain service model for implementing the endpoint and using the Kitchen View Controller Model for the UI implementation. I learned so much from working on this project including the unit and integration testing structure, when/how to use feature flags and experiments, and test-driven development (TDD). I also learned swift programming language basics such as delegates and dependency injection and Git for things like repositories and pipeline testing.\n\nImplemented this new recommended brands feature to show our users brands we think they would want to filter by based on their shopping and browsing history. We hope it has a substantial impact on our customer base and the way in which they shop and use the mobile iOS application. "
        },
        {
            "time": "1:30 PM - 1:45 PM",
            "projectId": "csse-3-130",
            "title": "Top to Bottom Networking",
            "studentName": "Joseph Collora",
            "studentMajor": "CSSE",
            "projectType": "Internship - Opanga Network",
            "facultyAdvisor": "Dr. Afra Mashhadi",
            "posterLink": "./posters/csse/collora_joseph_matthew.png",
            "abstract": "My project is an internship at the RAN optimization software company, “Opanga Networks.” The focus of the project is an SNMP management module for the company’s service/testing suite for deployment among other projects including a month in Scotland building a network for 80,000 attendees from the ground up as loaned labor with an event technology contractor, “Straight Up Technologies.” There, I was responsible for the physical deployment of Cisco Systems equipment in addition to an automatic switch configuration software for the 150th British Open golf tournament. \n\nThe first project, the SNMP management module, was implemented in a REST API that provides various services for their network optimizing and monitoring software products. By modifying their existing notification handler, adding some persistence entities and repositories, and creating HTTP endpoints to send and clear messages I was able to do this. Previously, SNMP notifications were handled through an existing system. The implementation was pretty barebones and lacked many important testing features. This de-coupled management system allows for monitoring of sent mock-SNMP messages sent during deployment. This solution makes it easy to deploy the optimization software for customers. This is because there are now HTTP endpoints for the customer SNMP manager to interact with to ensure a proper connection (in addition to monitoring licenses and other essential functionalities) and proper notification sending in addition to an independent database table to observe mocked messages. This will make testing notifications on customer deployments significantly easier.\n\nIn addition to being the SNMP manager, I physically deployed Cisco Systems hardware and fiber optic cable out in the field as a technician. I hung access points high underneath grandstands for attendee Wi-Fi and installed routers/switches throughout the course in enclosures to provide power to APs and connectivity inside administrative buildings. In addition to installing hardware, was a centralized Cisco switch configuration and logging software. This was important for the operation as there were hundreds of different switches deployed during the event and many issues occurred due to human involvement and script segmentation. With the new switch auto-configurator, deployment of switches completely avoided human error and had a smooth deployment for the event with performance that satisfied the R&A customer running the tournament."
        },
        {
            "time": "1:45 PM - 2:00 PM",
            "projectId": "csse-3-145",
            "title": "Health Data Visualization and Analysis: Gekko Corporation Software Development Intern",
            "studentName": "Jazminh Diep",
            "studentMajor": "CSSE",
            "projectType": "Internship - Gekko Corp.",
            "facultyAdvisor": "Dr. Afra Mashhadi",
            "posterLink": "./posters/csse/diep_jazminh.png",
            "abstract": "As a software development intern at Gekko Corporation, I focused on obtaining health data from smart health devices to help make health monitoring more efficient, accessible, and effective. The challenge of patient visits is the analysis the doctor provides is a 60-minute snapshot of health with gaps for the remaining over ½ million minutes in a year. The solution we are working on will address this gap as well as ensure an aggregate view of 24/7 health data to create baseline metrics for trending analysis; use machine learning to correlate how metrics such as elevated sugar levels impact BMI, sleep, and overall health; and assist health care providers with a broad view of the patient.\n\n I was assigned to build an application to pull data from wearable and IoT devices. The most efficient process was to leverage existing apps such as the Fitbit application to pull data from devices using Fitbit’s software development kit (SDK). The application I created pulled data from the Fitbit watch but the Fitbit’s SDK had limited functionality and the Splunk instance was not being called properly.\n\n After extensive research, I found an alternative solution using Fitbit's public web API that would allow seamless integration. Using the APIs, I discovered the Google Fit app with its REST API enables many smart health devices to be connected and would be a central way to capture various health devices. \n\nI created a script using Javascript that would call the Fitbit web API and Google Fit REST API to get the data types we want and then forward it to my Splunk instance. In Splunk, I created visualizations by querying the data using Splunk’s Search Processing Language (SPL). All the visualizations would be stored in one dashboard so that the user can see all health data in one main location. Lastly, machine learning was applied to data types for health professionals and patients to better understand the health data.\n\n I was able to aggregate the Fitbit Watch, Apple Watch, and Renpho smart scale data onto the health dashboard. Overall, I was able to collect a total of 2 weeks' worth of health data.   Having the correlated data, health dashboard, and machine learning analysis has allowed a proof of concept for my team to pitch to potential customers. "
        }, 
        {
            "time": "2:00 PM - 2:15 PM",
            "projectId": "csse-3-200",
            "title": "ECG Cataloguer - Creating and Analyzing Machine-Learning Datasets",
            "studentName": "Gary Guiragossian",
            "studentMajor": "CSSE",
            "projectType": "Internship - Bardy Diagnostics",
            "facultyAdvisor": "Dr. Michael Stiber",
            "posterLink": "./posters/csse/guiragossian_gary.jpg",
            "abstract": "The algorithms team at Bardy Diagnostics creates neural network models that generate health information predictors from medical device data. The algorithms employed deconstruct and notate ECG Data, marking irregularities and arrythmias. For these models to perform, they need separate data sets on which the models are trained, validated, and production tested. As it stands, the datasets in use by the team are manually curated. These sets are composed of groups of real-patient data that is carefully chosen to teach the neural networks to recognize a given arrythmia and/or sinus pattern. This process is laborious, time-intensive, and prone to misrepresentation of the patient population.  \n\nMy project aims to ease the dataset curation process by providing a toolset for managing our data pipeline. First, the module efficiently parses hardware files and collates them into a unified JSON file. In doing so, the files are prepared for a centralized, relational database that was created for the project. This database is securely managed and populated entirely through in-module connectors. Finally, the users may query the database through an abstracted interface of functions. These functions assist the user in creating datasets based on the parameterized filters they input. That created set of studies may then be analyzed against other datasets and/or the population to see if it is representative or if the parameter limits should be adjusted. This analysis relies on comparing distributions and hypothesis testing the datasets for factors of similarity. The analysis is output as a combined report of visualizations, tests, and general information on the dataset. "
        },
        {
            "time": "2:15 PM - 2:30 PM",
            "projectId": "csse-3-215",
            "title": "Using GIS Data for Model NG911",
            "studentName": "Alex Neary",
            "studentMajor": "CSSE",
            "projectType": "Faculty Research",
            "facultyAdvisor": "Dr. Michael Stiber",
            "posterLink": "./posters/csse/neary_alex.jpg",
            "abstract": "Graphitti is a graph-based event simulator that can facilitate the migration and validation of large-scale and long-duration simulations between CPUS and GPUs to achieve high performance. Emergency Services Communications Systems (ESCS) is an example of a large-scale model that naturally lends itself to be modeled using graphs. Next Generation 9-1-1 (NG9-1-1) is an initiative aimed at updating the ESCS infrastructure in the United States, and part of that initiative is the use of the Geographic Information System (GIS) data model. We are using this GIS data to create a graph representation of the NG9-1-1 system, using King County as an example. We will pass that graph in to the Graphitti simulator to configure the simulation. \n\nIn my project, I researched this system and available data sources and created a tool using Python that can read in GIS data, create a graph based on that data, and produce a file in GEXF (Graph Exchange XML Format) standard. The graph has three different types of nodes that represent different sections of the NG9-1-1 system: PSAP nodes (representing Public Safety Answering Points that receive calls and dispatch emergency services), Emergency Services nodes (such as fire or ambulance), and caller regions (which produce calls that are passed along to PSAPS).\n\n A challenge of this project was representing location information for caller regions. Since the graph only represents things relationally and we cannot pass in complex shape information to the simulator, we used a pixelated cartogram to adapt complex shapes into a series of simple square regions."
        },
        {
            "time": "2:30 PM - 2:45 PM",
            "projectId": "csse-3-230",
            "title": "WildStyle Gym Lead Manager",
            "studentName": "Sierra Goss",
            "studentMajor": "CSSE",
            "projectType": "Internship - Wild Style Gym",
            "facultyAdvisor": "Dr. Robert Dimpsey",
            "posterLink": "./posters/csse/goss_sierra.jpg",
            "abstract": "Introduction\n\nThe Customer Relations Manager is a software program that tracks the incoming leads for WildStyle Gym. Due to an increase in inquiries about programs, lead managers need a better system to track, update and automate interactions with each lead. \n\nGoal\n\n The software designed will store the contact information, notes and the pipeline stage of each inquiring lead. It will record all the data from leads that chose to join the gym including the membership they chose, the date joined and the total startup cost of each new member. The CRM will also automate emails to leads reminding them of their appointments, and create an automated task list for lead managers based on the stages of their leads. \n\nMethods\n\nThe WildStyle CRM was built with the PERN stack. The PERN stack uses PostgreSQL as the database to store data, Express as the framework for Node.js, React which is a JavaScript library that provides an efficient way to develop the client side or front-end of an app, and Node.js to create the server side of the application.\n\nI switched to the PERN stack application from using ReTool as the front end interface because of the limitations that ReTool has with data manipulation. \n\nResults\n\nThe Customer Relations Manager is able to track the incoming leads effectively. The features implemented are adding leads manually, updating and deleting leads, managing the pipeline stage of each lead, displaying and updating the closed leads table, and moving from page to page, defaulting to the homepage.\n\nI will continue to work on the CRM after my capstone in order to use it as a portfolio piece as I look for work."
        },
        {
            "time": "2:45 PM - 3:00 PM",
            "projectId": "csse-3-245",
            "title": "Autonomic Vacuuming of Redshift Databases",
            "studentName": "Douglas White",
            "studentMajor": "CSSE",
            "projectType": "Internship - Amazon",
            "facultyAdvisor": "Dr. Robert Dimpsey",
            "posterLink": "./posters/csse/white-douglas2.png",
            "abstract": "Storage constraints and input/output (I/O) operations are two of the most expensive aspects of database management.  Garbage collection is necessary in order to free up disk space when data is deleted.  If this process does not occur, it can become impossible for new data to become incorporated into the database.  Sorting data prevents fragmentation and reduces the number of I/O operations required when searching a table.  In order to alleviate the costs associated with storage and I/O operations, garbage collection and sorting should be run often.\n\nIn the Redshift service, sorting of new and modified data happens in the background, asynchronous from user workload.  This sorting achieves data clustering, reducing the amount of I/O performed when executing queries.  The sorting is also expensive in system resource consumption, and impacts the performance of any concurrently running workload.  Autonomic algorithms in the service evaluate the cost-benefit trade-off and determine which data sets to sort and when.  In extreme cases where user workload keeps the instance busy all the time, it can lead to starvation of the autonomic jobs.\n\nWhile working with Redshift’s autonomic team, I incorporated feature sets for vacuum queries into a machine learning algorithm to predict the run times of sorting and garbage collection queries.  Leveraging this data along with the predicted run times of a user’s query pre and post data clean-up, the system is now able to autonomically determine the most beneficent times at which to perform vacuum queries."
        }           
    ]
}